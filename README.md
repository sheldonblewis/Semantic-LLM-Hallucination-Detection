# Semantic-LLM-Hallucination-Detection
An application that uses semantic logic to detect hallucinations in LLM-generated text when contrasted against an input basis of ground truth.

First, train the model using "python model_tuning.py"
Then, run the server with "python detection.py"

Demo:
Part 1 - [https://www.loom.com/share/899b9a132feb44528b725c442691fb10?sid=843cac8b-1b8e-4fb0-b715-a4777ce06a43](https://www.loom.com/share/899b9a132feb44528b725c442691fb10?sid=93d072e0-7f83-41eb-ac17-8ae9b36540a7)
Part 2 - [https://www.loom.com/share/899b9a132feb44528b725c442691fb10?sid=843cac8b-1b8e-4fb0-b715-a4777ce06a43](https://www.loom.com/share/3daa92240433491a8ad4ae1e7c77ef55?sid=36d34746-fd43-4a6e-85e6-45ea87527e49)
Part 3 - https://www.loom.com/share/e31e101f823e445ab0741d3147627a34?sid=3346848f-8a72-4305-9be5-cd4e5b087574
